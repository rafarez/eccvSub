\documentclass[runningheads]{llncs}

%\usepackage{times}
%\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{caption}
%\usepackage{subfigure}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{ruler}
\usepackage{color}
%\usepackage{pgfplots}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
%\newtheorem{proposition}{Proposition}
%\newtheorem{lemma}{Lemma}
%\newtheorem{proof}{Proof}
\newcommand{\RR}{\mathbb R}
\newcommand{\NNN}{\mathcal N}
\newcommand{\sumi}{\displaystyle{\sum_{i=1}^n}}
\DeclareMathOperator*{\argmin}{argmin}

% Highlighting
\usepackage{soul}
\usepackage{color}
%\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\hlc}[2][yellow]{{\sethlcolor{#1}\hl{#2}}}
\newcommand{\RAF}[1]{\hlc[yellow]{(RR:) #1}}
\newcommand{\JZ}[1]{\hlc[pink]{(JZ:) #1}}
\newcommand{\PP}[1]{\hlc[green]{PP: #1}}

\usepackage{tikz}
\usepackage{pgfplots,pgfplotstable}
\usetikzlibrary{pgfplots.groupplots}
\pgfplotsset{grid=major,height=2in, width=\columnwidth}
\input{tikz_styles.tex}

%\input{test_tikz_data.tex}

\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers


\pagestyle{headings}
\mainmatter
\def\ECCV16SubNumber{1390}  % Insert your submission number here

\title{Kernel Squared Loss Exemplar Machines For Image Retrieval} % Replace with your title

\titlerunning{ECCV-16 submission ID \ECCV16SubNumber}

\authorrunning{ECCV-16 submission ID \ECCV16SubNumber}

\author{Anonymous ECCV submission}
\institute{Paper ID \ECCV16SubNumber}


\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
This paper proposes an extension to the exemplar SVM (ESVM) feature encoding pipeline first proposed by Zepeda and P\'erez \cite{ZePe15}. We first show that, by replacing the hinge loss by the square loss in the ESVM cost function, similar results in image retrieval can be obtained on a fraction of the computational cost. We call this model square loss exemplar machine, or SLEM. Secondly, we introduce a kernelized SLEM variant which benefits from the same computational advantages but displays improved performance. Both SLEM variants exploit the fact that the negative examples are biased for all the positives in the training set, so most of the SLEM computational complexity can be incurred offline by exploiting the Woodbury matrix identity. Our experiments establish the performance and computational advantages of our methods using a large array of state-of-the-art base feature representations, and well-known image retrieval dataset.
\end{abstract}

\input{sec_intro.tex}

%\input{sec_prior.tex}

\input{sec_SLEM.tex}

\input{sec_kernel_methods.tex}

\input{sec_eff_imp.tex}

\input{sec_eval.tex}

\input{comp_sota.tex}

\input{conclusion.tex}

\bibliographystyle{ieee} 
\bibliography{sup,jz}
\end{document}


	
%% Local Variables:
%% TeX-master: "main_eccv"
%% End:
