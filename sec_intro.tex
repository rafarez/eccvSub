\section{Introduction}
\JZ{To remove an error message related to the subcaption package, I set \textbackslash{}captionsetup\{compatibility=false\} 
Not sure what the consequences are....}
\JZ{Someone told me the captions should be above the tables... To check in author's kit, as its not our case.}

Robust image representations are a crucial component of a vast number of computer vision applications. Within these, the \emph{image retrieval} application is an important example wherein a query image is provided to the system, which must in turn find all matching images from within a large, unannotated database. The matching process is done entirely based on the pixel content of the query and database images, and the search must be robust to large image variations due to camera pose, color differences and scene illumination, amongst others.

The success of several existing systems in this challenging application has indeed been enabled by advances in  image representation functions. An image representation function commonly maps a given input image to a vector in a fixed dimensional space called the image \emph{feature vector}. The image retrieval process then reduces to finding the feature vectors closest to the query image feature vector under {\it e.g.,} the Euclidean distance. An adequate image representation function must hence produce feature vectors that are robust to image variations, while at the same time incurring low feature extraction computational overhead.

A now pervasive example of an image feature representation is that consisting of the activation coefficients extracted from the previous-to-last layer of Convolutional Neural Networks (CNNs) \cite{Krizhevsky2012}. Although CNNs are trained in a fully-supervised manner for the image classification task, they have been shown to transfer well not only to new, unseen classes \cite{Oquaba,Chatfield2014}, but also to alternate tasks such as object detection \cite{Girshick2014} and, importantly, image retrieval \cite{Sharif}.

Yet the more successful current feature representations for image retrieval rely on unsupervised models such as $K$-means \cite{Delhumeau2013} or Gaussian Mixture Models \cite{Perronnin2010}. Very few methods \cite{Arandjelovic15,Rana,Bilen2015} exist that exploit supervised learning of image features directly for the image retrieval task. One of the main reasons for this is the lack of adequately large and varied supervised datasets that are expensive to collect.

The Exemplar Support Vector Machine (ESVM) formulation originally proposed by Malisiewicz {\it et al.} \cite{Malisiewicza} is a way to leverage the availability of large, unannotated pools of images within the context of supervised learning. The approach consists of using a large generic pool as a set of negative examples, while using a single image (the \emph{exemplar}) as a positive example. Given these training set, a linear classifier is learned that can  generalize well, despite the drastically limited size of the set of positive examples. 

An extension \cite{ZePe15} of the ESVM formulation discussed above instead treats the resulting linear classifier as a new feature vector for the exemplar image. %This new ESVM feature is derived from \emph{base} feature representations ({\it e.g.,} CNN activation features) of the exemplar and the negative pool.
%An extension \cite{ZePe15} of the ESVM formulation discussed above instead treats the resulting linear classifier as an \emph{enhanced} representation of the image feature representing the exemplar image (the \emph{base} feature representation).
An ESVM feature is hence extracted from each database image, as well as from the query image, by treating each image as an exemplar while keeping a fixed pool of generic negative images. Searching hence amounts to computing distances between the query ESVM feature and the databse ESVM features. Note that ESVM features can be derived from arbitrary \emph{base} feature representations ({\it e.g.} CNN activation features) of the exemplar and the images in the generic negative pool. An interesting interpretation of this approach is that the ESVM features represent what is unique about the exemplar relative to the pool of generic negatives, as this is the role of a linear classifier.

One important drawback of the ESVM feature encoding approach of \cite{ZePe15} is that computing the linear classifier requires implementing an iterative Stochastic Gradient Descent (SGD) solver, and this can be time consuming for large negative pool sizes required for good ESVM feature performance. The need for an SGD-based solution is a consequence of the hinge loss appearing in maximum margin SVM objetives. Hence, in this work we  propose using the squared error instead of the hinge loss, in effect converting the ESVM problem into a ridge regression problem that can be solved in closed form. The resulting feature, which we call the Squared Loss Exemplar Machine (SLEM) feature, is shown experimentially to incur drastically reduced computation time relative to the ESVM feature of \cite{ZePe15}. 

Since computing the SLEM feature requires inverting a large matrix related to the training set's scatter matrix, we propose an efficient way to compute this inverse that exploits the fact that only a single example (the exemplar) changes in the training set when computing SLEM features for different images. Our approach exploits the Woodrow identity to express the inverse matrix in terms of the inverse of a sub-matrix related to the generic negative pool, as this sub-matrix inverse can be computed offline. Despite the resulting computational advantages of the proposed SLEM feature, we further establish experimentally that our representation can match and even improve upon the performance of ESVM features on two well known datasets and when using a wide range of base features. 

We further introduce a non-linear kernel variant of SLEM that enjoys even greater performance advantages. We demonstrate analytically that one can exploit the representer theorem to obtain a solution of the non-linear SLEM variant that uses the same closed-form solution used for linear SLEM. Hence, non-linear SLEM enjoys computational advantages comparable to those of linear SLEM, while benefitting from increased image retrieval performance, and we establish both of these points experimentally. In order to enjoy further computational benefits for the non-linear SLEM case, we also propose using a low-rank Cholesky approximation of the negative pool's  kernel matrix \cite{Bach}.

The rest of this paper is organized as follows: 
%In Section \ref{prior work} we provide an overview of various existing feature representation methods. 
In Section \ref{lsesvm} we first review the original ESVM feature representation method and subsequently introduce the proposed linear SLEM methodd. We then introduce the non-linear SLEM variant in Section \ref{nonlinear SLEM}. We then present the low-rank approximation of our method that enables efficient implementations in the non-linear case in  Section \ref{eff_imp}. We evaluate our proposed method in image retrieval in Section \ref{eval}, and present conclusions in Section \ref{conclusion}.


% Image search
% - Importance.
% - Feature representations. 

% Feature representation - supervised learning
% - Generic framework.
% - Deep CNN features.
% - ImageNet.

% Image retrieval
% - Supervised - Eusipco, NetVLAD.
% - VLAD, Fisher.
% - Cheaper.

% Pool:
% LDA
% SVMs for feature representations - parts learning, Malizsiewics


%\section {Introduction}
% The exemplar SVM (E-SVM) was first introduced by Malisiewicz et al. in \cite{Efros11} as a conceptually simple framework for object detection and image classification where the training set has a small ratio of positive/negative examples. 
% At training time, many SVMs are learned from a large pool of negative against a single positive (so called exemplar). 
% At test time, the scores of the test images for each classifier are fitted by a logistic regression. 
% The final score of an image is then a non-linear combination of scores from multiples exemplar SVMs.

% They have also been used in \cite{Efros12} in image retrieval tasks, using the classifier score to rank matching candidates.
% However this transfer of information from classification to retrieval is severely limited. 
% Indeed, the purpose of a support vector machine is to separate positive and negative samples, i.e., predict discrete labels. 
% The distance between a negative sample and the classification hyperplane has no value as a measure of has \emph{a priori} no value of continuous matching score.

% Zepeda et al. address this problem in \cite{ZePe15} by firstly, performing an E-SVM to each image in a dataset instead of only for the query images and secondly, comparing its classifiers distance to the query's classifier instead of comparing scores. 
% These modifications guarantee we are ranking distances between two points instead of classification scores.
% Therefore \cite{ZePe15} refers to E-SVMs as features encoders: a pipeline that takes an image representation as input and returns an improved image representation. This type of feature enhancing is more akin to methods such as whitening, PCA and LDA.

% This paper introduces the square-loss Exemplar machine (SLEM), which consists of optimizing the same cost function of a regular E-SVM where the hinge loss is replaced by the square loss. 
% The square loss version has the advantage of having a much more efficient optimization. Indeed, the minimization of its cost function is solved by a linear system and can be done for all exemplar simultaneously, whereas the regular E-SVM cost function is generally minimized one exemplar at a time, normally by stochastic gradient descent \cite{bottou10}.
% Also, for the machine learning tasks of binary classification, both regular SVM and least-squares SVM have similar performances \cite{YeXi07}.
% %Also, for the machine learning tasks of binary classification, both regular SVM and least-squares SVM have the same performance if positive and negative samples are separable and the pool of samples is linearly independent \cite{YeXi07}. 
% We also introduce a kernelized version of SLEM, efficiently implemented, that gives better results and superior scalability for large-scale image retrieval problems.

%%% Local Variables:
%%% TeX-master: "main_eccv"
%%% End: